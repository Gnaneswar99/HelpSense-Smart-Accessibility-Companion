# <img width="70" height="71" alt="image" src="https://github.com/user-attachments/assets/9b0e690b-4a57-4d1e-be8c-30960c0bd902" /> HelpSense â€” Smart Accessibility Companion


<img width="500" height="500" alt="image" src="https://github.com/user-attachments/assets/bc28d5fc-b02b-4b9b-aed9-4a4589cce5b7" />
<img width="500" height="500" alt="image" src="https://github.com/user-attachments/assets/e08322ef-8569-48a7-8fa8-8013df3be077" />


An on-device AI accessibility app built with **Kotlin Multiplatform (KMM)** â€” helping people with visual, hearing, and mobility impairments navigate the world through real-time image captioning, sound classification, accessible navigation, and communication tools.

> **All AI runs on-device** â€” no internet required, no data leaves your phone.

## ğŸ“± Features

### ğŸ‘ï¸ Vision
- Real-time **object detection** (MobileNet SSD) with spatial descriptions
- **Image captioning** â€” natural language scene descriptions ("I see a park with a person on the left and a dog nearby")
- **CameraX** pipeline optimized for ML inference
- Text-to-Speech output of all captions

### ğŸ”Š Audio Monitor
- **YAMNet** sound classification (521 classes â†’ 15 accessibility-relevant categories)
- **Priority-based alerts**: fire alarm â†’ heavy haptics + interrupt TTS, doorbell â†’ medium haptics
- 16kHz audio capture pipeline optimized for real-time ML

### ğŸ§­ Navigation
- **Accessible route planning** with step-by-step voice guidance
- **Obstacle detection** during navigation using live camera feed
- Distance estimation and directional warnings ("Car nearby to your left")
- Progress tracking with previous/next/repeat controls

### ğŸ’¬ Communication
- **35+ Quick Phrases** across 6 categories (Greetings, Needs, Emergency, Directions, Shopping, Medical)
- **Text-to-Speech** â€” type any message and speak it aloud
- **Speech-to-Text** â€” capture what others say and display on screen
- **Conversation log** â€” two-way communication for deaf/hard-of-hearing users

## ğŸ¯ What Makes This Special

- **Cross-platform** â€” Kotlin Multiplatform shared business logic for Android (iOS ready)
- **On-device ML** â€” TensorFlow Lite inference, zero cloud dependency
- **Accessibility-first** â€” TalkBack support, live regions, semantic headings, 48dp+ touch targets
- **Clean Architecture** â€” MVVM + Use Cases + Repository pattern + Koin DI
- **Production patterns** â€” Coroutines, Flow, proper lifecycle management

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Presentation Layer             â”‚
â”‚  Jetpack Compose + Material 3            â”‚
â”‚  ViewModels + StateFlow                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Domain Layer (KMM)            â”‚
â”‚  Use Cases + Repository Interfaces       â”‚
â”‚  Models + Business Logic                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Data Layer                  â”‚
â”‚  TFLite ML Engines + Platform Services   â”‚
â”‚  Camera, Audio, TTS, Speech Recognition  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| Language | Kotlin 1.9.22 (Multiplatform) |
| Android UI | Jetpack Compose + Material 3 |
| DI | Koin 3.5.3 |
| ML | TensorFlow Lite 2.14.0 |
| Camera | CameraX 1.3.1 |
| Audio | AudioRecord (16kHz PCM) |
| Speech | Android SpeechRecognizer + TTS |
| Async | Kotlin Coroutines + Flow |
| Navigation | Compose Navigation |

## ğŸ“‚ Project Structure

```
HelpSense/
â”œâ”€â”€ shared/                       # KMM shared module
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ commonMain/           # Cross-platform code
â”‚       â”‚   â”œâ”€â”€ core/model/       # Domain models
â”‚       â”‚   â”œâ”€â”€ core/ml/          # ML engine interfaces
â”‚       â”‚   â”œâ”€â”€ core/util/        # Service interfaces (TTS, Camera, Audio)
â”‚       â”‚   â”œâ”€â”€ core/di/          # Koin modules
â”‚       â”‚   â””â”€â”€ feature/          # Use cases per module
â”‚       â”‚       â”œâ”€â”€ vision/
â”‚       â”‚       â”œâ”€â”€ audio/
â”‚       â”‚       â”œâ”€â”€ navigation/
â”‚       â”‚       â””â”€â”€ communication/
â”‚       â”œâ”€â”€ androidMain/          # Android actual implementations
â”‚       â””â”€â”€ commonTest/           # Unit tests
â”œâ”€â”€ androidApp/                   # Android application
â”‚   â””â”€â”€ src/main/
â”‚       â”œâ”€â”€ java/.../android/
â”‚       â”‚   â”œâ”€â”€ ml/               # TFLite engines & repositories
â”‚       â”‚   â”œâ”€â”€ service/          # Platform services
â”‚       â”‚   â”œâ”€â”€ ui/screens/       # Compose screens per module
â”‚       â”‚   â””â”€â”€ di/               # Android Koin module
â”‚       â””â”€â”€ assets/models/        # TFLite model files
â””â”€â”€ buildSrc/                     # Centralized dependency versions
```

## ğŸš€ Getting Started

### Prerequisites
- Android Studio Hedgehog (2023.1.1) or newer
- JDK 17
- Android SDK 34

### Setup
```bash
git clone https://github.com/YOUR_USERNAME/HelpSense.git
```
1. Open `HelpSense/` in Android Studio
2. Wait for Gradle sync to complete
3. Run on a physical device or emulator (API 26+)

### ML Models (Optional)
The app works **without model files** â€” ML features show informative messages while all UI, navigation, and communication features work fully.

To enable Vision and Audio ML features, download models to `androidApp/src/main/assets/models/`:

| Model | File | Size | Source |
|-------|------|------|--------|
| Object Detection | `detect.tflite` + `labelmap.txt` | ~4 MB | [TFLite Model Zoo](https://www.tensorflow.org/lite/examples/object_detection/overview) |
| Image Classification | `label.tflite` | ~3 MB | [MobileNet v2](https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz) |
| Sound Classification | `yamnet.tflite` + `yamnet_labels.txt` | ~3 MB | [YAMNet on Kaggle](https://www.kaggle.com/models/google/yamnet/tfLite/classification-tflite) |

See [`MODEL_SETUP.md`](androidApp/src/main/assets/models/MODEL_SETUP.md) for detailed download instructions.

## ğŸ§ª Testing

```bash
# Shared unit tests
./gradlew :shared:testDebugUnitTest

# Android tests
./gradlew :androidApp:testDebugUnitTest
```

Test coverage: Domain models, Use Cases, Navigation Session, Communication Phrases, ML Engine interfaces

## ğŸ—ºï¸ Roadmap

- [x] KMM Project Structure & Gradle Setup
- [x] Core Accessibility Framework (TTS, Haptics, Camera, Audio)
- [x] ML Model Integration (Object Detection, Captioning, Sound Classification)
- [x] Navigation Module (Route Planning, Obstacle Detection, Voice Guidance)
- [x] Communication Module (Quick Phrases, STT, TTS, Conversation Log)
- [ ] iOS Implementation (SwiftUI screens, CoreML engines)
- [ ] Accessibility Audit (TalkBack testing, WCAG 2.1 AA compliance)

## ğŸ“„ License

MIT License â€” Built as a portfolio project demonstrating cross-platform mobile development with on-device AI.
